# Empirical Evaluation on Current Neural Networks on Cloze-style Reading Comprehension

---
**Note that, this repository will be updated irregularly**

##Introduction



##Related Papers
You can either download from this repository or in the following links.
> (Hermann et al., 2015) Teaching Machines to Read and Comprehend

http://arxiv.org/abs/1506.03340
> (Hill et al., 2015) The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations

http://arxiv.org/abs/1511.02301
> (Kadlec et al., 2016) Text Understanding with the Attention Sum Reader Network

http://arxiv.org/abs/1603.01547
> (Chen et al., 2016) A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task

https://arxiv.org/abs/1606.02858
> (Dhingra et al., 2016) Gated-Attention Readers for Text Comprehension

https://arxiv.org/abs/1606.01549
> (Sordoni et al., 2016) Iterative Alternating Neural Attention for Machine Reading

https://arxiv.org/abs/1606.02245
> (Trischler et al., 2016) Natural Language Comprehension with the EpiReader

https://arxiv.org/abs/1606.02270
> (Cui et al., 2016) Consensus Attention-based Neural Networks for Chinese Reading Comprehension

https://arxiv.org/abs/1607.02250
> (Cui et al., 2016) Attention-over-Attention Neural Networks for Reading Comprehension

https://arxiv.org/abs/1607.04423
> (Li et al., 2016) Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering

https://arxiv.org/abs/1607.06275
